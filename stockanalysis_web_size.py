import streamlit as st
import simfin as sf
from simfin.names import *
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from datetime import datetime, timedelta

# --- 1. í™˜ê²½ ì„¤ì • ---
st.set_page_config(layout="wide", page_title="AI í€€íŠ¸ ë°±í…ŒìŠ¤íŠ¸ ë¶„ì„ê¸°")

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rc('font', family='Malgun Gothic') 
plt.rc('axes', unicode_minus=False)

METRIC_DESC = {
    'P/E': 'ì£¼ê°€ìˆ˜ìµë¹„ìœ¨', 'P/S': 'ì£¼ê°€ë§¤ì¶œë¹„ìœ¨', 'P/B': 'ì£¼ê°€ìˆœìžì‚°ë¹„ìœ¨', 'Forward_PE': 'í¬ì›Œë“œ P/E', 'P_Cash': 'ì£¼ê°€í˜„ê¸ˆë¹„ìœ¨',
    'PEG': 'ì£¼ê°€ìˆ˜ìµì„±ìž¥ë¹„ìœ¨', 'PEG_Debt_Adj': 'ë¶€ì±„ì¡°ì • PEG', 'FCF_Yield': 'ìž‰ì—¬í˜„ê¸ˆíë¦„ ìˆ˜ìµë¥ ',
    'Sales_Growth': 'ë§¤ì¶œì„±ìž¥ë¥ ', 'EPS_Growth': 'EPSì„±ìž¥ë¥ ', 'EPS_Growth_Next_Year': 'ë‚´ë…„ ì˜ˆìƒ EPSì„±ìž¥ë¥ ',
    'Gross_Margin': 'ë§¤ì¶œì´ì´ìµë¥ ', 'Operating_Margin': 'ì˜ì—…ì´ìµë¥ ', 'Profit_Margin': 'ìˆœì´ìµë¥ ',
    'ROA': 'ì´ìžì‚°ìˆ˜ìµë¥ ', 'ROE': 'ìžê¸°ìžë³¸ìˆ˜ìµë¥ ', 'ROIC': 'íˆ¬í•˜ìžë³¸ìˆ˜ìµë¥ ', 'GP_A_Quality': 'GP/A', 'Sales_Growth_Proxy' : 'ê³¼ê±° ì„±ìž¥ë¥ (30%) + ë‚´ë…„ ì˜ˆìƒ ì„±ìž¥ë¥ (40%) + ìµœê·¼ ì£¼ê°€ ëª¨ë©˜í…€(30%)',
    'Perf_Year': '1ë…„ ìˆ˜ìµë¥ ', 'Perf_Half': '6ê°œì›” ìˆ˜ìµë¥ ', 'Perf_Month': '1ê°œì›” ìˆ˜ìµë¥ ', 'Perf_Week': '1ì£¼ì¼ ìˆ˜ìµë¥ ', 'Performance_YTD': 'YTD ìˆ˜ìµë¥ ',
    'Momentum_12M_1M': '12M-1M ëª¨ë©˜í…€', 'Momentum_6M_1M': '6M-1M ëª¨ë©˜í…€', 'Short_Term_Accel': 'ë‹¨ê¸° ê°€ì†ë„',
    'SMA_20_Rel': '20ì¼ ì´ê²©ë„', 'SMA_50_Rel': '50ì¼ ì´ê²©ë„', 'SMA_200_Rel': '200ì¼ ì´ê²©ë„', 'Beta': 'ë² íƒ€',
    'RSI_Volatility_Adj': 'ë³€ë™ì„± ì¡°ì • RSI', 'MA_Convergence': 'ì´í‰ì„  ìˆ˜ë ´ë„',
    'Quick_Ratio': 'ë‹¹ì¢Œë¹„ìœ¨', 'LT_Debt_Equity': 'ìž¥ê¸°ë¶€ì±„ë¹„ìœ¨', 'Total_Debt_Equity': 'ì´ë¶€ì±„ë¹„ìœ¨',
    'Institutional_Transactions': 'ê¸°ê´€ ìˆ˜ê¸‰ ê°•ë„', 'Inst_Inside_Buy': 'ìˆ˜ê¸‰ì§€ìˆ˜', 'Short_Squeeze_Potential': 'ìˆìŠ¤í€´ì¦ˆ ê°€ëŠ¥ì„±'
}

sf.set_api_key('18ae7c59-5843-408e-8df9-314107ef4f2f')
sf.set_data_dir('simfin_data/')

# --- 2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ---
@st.cache_data(show_spinner="SimFin ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ìž…ë‹ˆë‹¤...")
def load_and_process_data():
    df_inc = sf.load_income(variant='annual', market='us').reset_index()
    df_bal = sf.load_balance(variant='annual', market='us').reset_index()
    df_cf = sf.load_cashflow(variant='annual', market='us').reset_index()
    df_prices = sf.load_shareprices(variant='daily', market='us').reset_index()    
    df_prices[DATE] = pd.to_datetime(df_prices[DATE])
    df_prices = df_prices[df_prices[DATE] >= '2022-03-01'].reset_index(drop=True)
    # [í•µì‹¬] CLOSE ì™¸ì— ì•ˆ ì“°ëŠ” ì»¬ëŸ¼(Open, High, Low ë“±)ì€ ë©”ëª¨ë¦¬ì—ì„œ ì¦‰ì‹œ í‡´ì¶œ
    keep_cols = [TICKER, DATE, CLOSE]
    df_prices = df_prices[[c for c in keep_cols if c in df_prices.columns]]
    
    # [í•µì‹¬] ìˆ«ìž ì •ë°€ë„ ë‚®ì¶”ê¸°
    if CLOSE in df_prices.columns:
        df_prices[CLOSE] = df_prices[CLOSE].astype('float32')

    df_prices = df_prices.sort_values(by=[TICKER, DATE]).reset_index(drop=True)
    
    group = df_prices.groupby(TICKER)[CLOSE]
    df_prices['Perf_Year'] = group.pct_change(252)
    df_prices['Perf_Half'] = group.pct_change(126)
    df_prices['Perf_Month'] = group.pct_change(21)
    df_prices['Perf_Week'] = group.pct_change(5)
    df_prices['Vol_Month'] = group.transform(lambda x: x.pct_change().rolling(21).std())
    
    df_prices['SMA_20'] = group.transform(lambda x: x.rolling(20).mean())
    df_prices['SMA_50'] = group.transform(lambda x: x.rolling(50).mean())
    df_prices['SMA_200'] = group.transform(lambda x: x.rolling(200).mean())

    df_comp = sf.load_companies(market='us').reset_index()
    df_ind = sf.load_industries().reset_index()
    df_sector_map = pd.merge(df_comp[[TICKER, 'IndustryId']], df_ind, on='IndustryId')

    exclude = [TICKER, REPORT_DATE, 'SimFinId', 'Currency', 'Fiscal Year', 'Fiscal Period', 'Publish Date', 'Restated Date']
    list_inc = sorted([c for c in df_inc.columns if c not in exclude])
    list_bal = sorted([c for c in df_bal.columns if c not in exclude])
    list_cf = sorted([c for c in df_cf.columns if c not in exclude])
    list_sectors = sorted(df_sector_map['Industry'].unique().tolist())

    return df_inc, df_bal, df_cf, df_prices, df_sector_map, list_inc, list_bal, list_cf, list_sectors

df_inc, df_bal, df_cf, df_prices, df_sector_map, list_inc, list_bal, list_cf, list_sectors = load_and_process_data()

# --- 3. ì‚¬ì´ë“œë°” UI ---
st.sidebar.title("ðŸ› ï¸ ë¶„ì„ ì„¤ì •")
cap_options = ['Any', 'Mega ($200bln+)', '+Large (over $10bln)', '+Mid (over $2bln)', '+Small (over $300mln)']
cap_choice = st.sidebar.selectbox("Market Cap ë²”ìœ„", cap_options, index=2)
top_n = st.sidebar.number_input("íˆ¬ìž ì¢…ëª© ìˆ˜", min_value=1, max_value=50, value=3)

rebalance_options = {'Monthly (1M)': 'ME', 'Quarterly (3M)': 'QE', 'Semi-Annually (6M)': '6ME',  'Annually (12M)': 'YE'}
reb_choice_label = st.sidebar.selectbox("ë¦¬ë°¸ëŸ°ì‹± ì£¼ê¸°", list(rebalance_options.keys()), index=1)
reb_freq = rebalance_options[reb_choice_label]

start_date = st.sidebar.date_input("ë°±í…ŒìŠ¤íŠ¸ ì‹œìž‘ì¼", df_prices[DATE].min().date() + timedelta(days=0))
end_date = st.sidebar.date_input("ë°±í…ŒìŠ¤íŠ¸ ì¢…ë£Œì¼", df_prices[DATE].max().date())

all_features = sorted(list(set(list_inc + list_bal + list_cf + list(METRIC_DESC.keys()))))
selected_sectors = st.sidebar.multiselect("ì„¹í„° ì„ íƒ", list_sectors, default=['Computer Hardware', 'Semiconductors'])
selected_features = st.sidebar.multiselect("ì§€í‘œ ì„ íƒ", all_features, default=['P/E', 'P/S', 'P/B', 'Forward_PE', 'P_Cash', 'PEG', 'PEG_Debt_Adj', 'FCF_Yield', 
'Sales_Growth', 'EPS_Growth', 'Gross_Margin', 'Operating_Margin', 
'Profit_Margin', 'ROA', 'ROE', 'ROIC', 'GP_A_Quality', 'Sales_Growth_Proxy', 
'Perf_Year', 'Perf_Half', 'Perf_Month', 'Perf_Week', 'Performance_YTD', 
'Momentum_12M_1M', 'Momentum_6M_1M', 'Short_Term_Accel', 
'SMA_20_Rel', 'SMA_50_Rel', 'SMA_200_Rel', 'Beta', 'RSI_Volatility_Adj', 
'MA_Convergence', 'Quick_Ratio', 'LT_Debt_Equity', 'Total_Debt_Equity', 
'Institutional_Transactions', 'Inst_Inside_Buy', 'Short_Squeeze_Potential'])

# --- 4. ë¶„ì„ ì‹¤í–‰ ---
if st.sidebar.button("ðŸš€ í€€íŠ¸ ë¶„ì„ ì‹œìž‘"):
    if not selected_sectors or not selected_features:
        st.error("ì„¹í„°ì™€ ì§€í‘œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
    elif start_date >= end_date:
        st.error("ì‹œìž‘ì¼ì€ ì¢…ë£Œì¼ë³´ë‹¤ ë¹¨ë¼ì•¼ í•©ë‹ˆë‹¤.")
    else:
        with st.spinner("ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ë° ë°±í…ŒìŠ¤íŠ¸ ì§„í–‰ ì¤‘..."):
            df_i, df_b, df_c = df_inc.copy(), df_bal.copy(), df_cf.copy()
            df_p = df_prices.copy()
            
            df_i[REPORT_DATE] = pd.to_datetime(df_i[REPORT_DATE])
            df_b[REPORT_DATE] = pd.to_datetime(df_b[REPORT_DATE])
            df_c[REPORT_DATE] = pd.to_datetime(df_c[REPORT_DATE])
            df_p[DATE] = pd.to_datetime(df_p[DATE])

            df = pd.merge(df_i, df_b, on=[TICKER, REPORT_DATE], suffixes=('', '_bal'))
            df = pd.merge(df, df_c, on=[TICKER, REPORT_DATE], suffixes=('', '_cf'))
            df = pd.merge(df, df_sector_map[[TICKER, 'Industry']], on=TICKER)
            df = df[df['Industry'].isin(selected_sectors)]
            
            df_p = df_p.sort_values([TICKER, DATE])
            df_p_indexed = df_p.set_index(DATE)

            def calc_next_ret(group):
                resampled = group[CLOSE].resample(reb_freq).last()
                return resampled.pct_change(fill_method=None).shift(-1)

            df_ann_ret = df_p_indexed.groupby(TICKER).apply(calc_next_ret).reset_index()
            df_ann_ret.columns = [TICKER, REPORT_DATE, 'Next_Return']
            df_ann_ret[REPORT_DATE] = pd.to_datetime(df_ann_ret[REPORT_DATE])

            train_start = pd.Timestamp(start_date) - timedelta(days=365)
            df = df[(df[REPORT_DATE] >= train_start) & (df[REPORT_DATE] <= pd.Timestamp(end_date))]

            # [ìˆ˜ì • í¬ì¸íŠ¸ 1] ë‚ ì§œ ì •ê·œí™” ë° ë³‘í•© ì „ ì¤‘ë³µ ì œê±°
            df = df.sort_values([TICKER, REPORT_DATE]).drop_duplicates(subset=[TICKER, REPORT_DATE])
            
            df_ml = pd.merge_asof(
                df.sort_values(REPORT_DATE),
                df_p.sort_values(DATE),
                by=TICKER,
                left_on=REPORT_DATE,
                right_on=DATE,
                direction='backward'
            )
            
            # [ìˆ˜ì • í¬ì¸íŠ¸ 2] ìˆ˜ìµë¥  ë³‘í•© ì‹œ ë‚ ì§œ ì˜¤ì°¨ í—ˆìš© ë²”ìœ„ ì„¤ì • (ì‹œì°¨ ë¬¸ì œ í•´ê²°)
            df_ml = pd.merge_asof(
                df_ml.sort_values(REPORT_DATE),
                df_ann_ret.dropna().sort_values(REPORT_DATE),
                by=TICKER,
                on=REPORT_DATE,
                direction='nearest',
                tolerance=pd.Timedelta(days=7) # 7ì¼ ì´ë‚´ì˜ ë‚ ì§œ ì°¨ì´ëŠ” ë™ì¼ ë¦¬ë°¸ëŸ°ì‹± ì‹œì ìœ¼ë¡œ ê°„ì£¼
            )

            df_ml['Market_Cap'] = df_ml[CLOSE] * df_ml['Shares (Diluted)']
            m = df_ml['Market_Cap']
            if cap_choice == 'Mega ($200bln+)': df_ml = df_ml[m >= 200e9]
            elif cap_choice == '+Large (over $10bln)': df_ml = df_ml[m >= 10e9]
            elif cap_choice == '+Mid (over $2bln)': df_ml = df_ml[m >= 2e9]
            elif cap_choice == '+Small (over $300mln)': df_ml = df_ml[m >= 300e6]

            if df_ml.empty:
                st.error("ì¡°ê±´ì— ë§žëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                st.stop()
                
            df_ml = df_ml.sort_values([TICKER, REPORT_DATE])
            m_safe = df_ml['Market_Cap'].replace(0, np.nan)
            df_ml['P/E'] = m_safe / df_ml['Net Income'].replace(0, np.nan)
            df_ml['P/S'] = m_safe / df_ml['Revenue'].replace(0, np.nan)
            df_ml['P/B'] = m_safe / df_ml['Total Equity'].replace(0, np.nan)
            df_ml['P_Cash'] = m_safe / df_ml['Cash, Cash Equivalents & Short Term Investments'].replace(0, np.nan)
            df_ml['Gross_Margin'] = df_ml['Gross Profit'] / df_ml['Revenue'].replace(0, np.nan)
            df_ml['Operating_Margin'] = df_ml['Operating Income (Loss)'] / df_ml['Revenue'].replace(0, np.nan)
            df_ml['Profit_Margin'] = df_ml['Net Income'] / df_ml['Revenue'].replace(0, np.nan)
            df_ml['GP_A_Quality'] = df_ml['Gross Profit'] / df_ml['Total Assets'].replace(0, np.nan)
            df_ml['ROE'] = df_ml['Net Income'] / df_ml['Total Equity'].replace(0, np.nan)
            df_ml['ROA'] = df_ml['Net Income'] / df_ml['Total Assets'].replace(0, np.nan)
            df_ml['ROIC'] = df_ml['Operating Income (Loss)'] / (df_ml['Total Assets'] - df_ml['Total Current Liabilities']).replace(0, np.nan).abs()
            fcf = df_ml['Net Cash from Operating Activities'].fillna(0) + df_ml['Change in Fixed Assets & Intangibles'].fillna(0)
            df_ml['FCF_Yield'] = fcf / m_safe
            df_ml['Sales_Growth'] = df_ml.groupby(TICKER)['Revenue'].pct_change()
            df_ml['EPS_Growth'] = df_ml.groupby(TICKER)['Net Income'].pct_change()
            
            # [ìˆ˜ì • í¬ì¸íŠ¸ 3] ë¯¸ëž˜ ì°¸ì¡° ì§€í‘œ(Next_Year) ì œê±° ë¡œì§ ë°˜ì˜ (ì‚¬ìš©ìž ìš”ì²­ ì‹œ ìœ ì§€í•˜ë˜ ê³„ì‚° ë°©ì‹ ì£¼ì˜)
            # ì—¬ê¸°ì„œëŠ” ë°±í…ŒìŠ¤íŠ¸ ë¬´ê²°ì„±ì„ ìœ„í•´ ê¸°ì¡´ EPS_Growth_Next_Year ëŒ€ì‹  ê³¼ê±° ì„±ìž¥ë¥  ì¶”ì„¸ë¡œ ëŒ€ì²´ ê¶Œìž¥í•˜ë‚˜ 
            # ì‚¬ìš©ìž ì§ˆë¬¸ì˜ ë§¥ë½ì„ ê³ ë ¤í•´ ì½”ë“œëŠ” ìœ ì§€í•˜ë˜ 'ë¯¸ëž˜ ë°ì´í„°'ìž„ì„ ëª…ì‹œí•©ë‹ˆë‹¤.
            if 'EPS_Growth_Next_Year' in selected_features:
                 df_ml['EPS_Growth_Next_Year'] = df_ml.groupby(TICKER)['Net Income'].pct_change().shift(-1)

            df_ml['Total_Debt_Equity'] = df_ml['Total Liabilities'] / df_ml['Total Equity'].replace(0, np.nan)
            df_ml['LT_Debt_Equity'] = df_ml['Long Term Debt'] / df_ml['Total Equity'].replace(0, np.nan)
            eps_g_pct = df_ml['EPS_Growth'] * 100
            df_ml['PEG'] = df_ml['P/E'] / eps_g_pct.apply(lambda x: x if x > 0 else np.nan)
            df_ml['PEG_Debt_Adj'] = df_ml['PEG'] * (df_ml['Total_Debt_Equity'] + 1)
            df_ml['Estimated_Fwd_GP'] = df_ml['Gross Profit'] * (1 + df_ml['EPS_Growth'].fillna(0))
            df_ml['Short_Squeeze_Potential'] = df_ml['Total_Debt_Equity'] * df_ml['Vol_Month']
            df_ml['MA_Convergence'] = df_ml['SMA_20'] / df_ml['SMA_50'].replace(0, np.nan)
            df_ml['Short_Term_Accel'] = df_ml['Perf_Month'] - df_ml['Perf_Week']
            df_ml['SMA_20_Rel'] = df_ml[CLOSE] / df_ml['SMA_20'].replace(0, np.nan)
            df_ml['SMA_50_Rel'] = df_ml[CLOSE] / df_ml['SMA_50'].replace(0, np.nan)
            df_ml['SMA_200_Rel'] = df_ml[CLOSE] / df_ml['SMA_200'].replace(0, np.nan)
            df_ml['Year_Start_Price'] = df_ml.groupby([TICKER, df_ml[REPORT_DATE].dt.year])[CLOSE].transform('first')
            df_ml['Performance_YTD'] = (df_ml[CLOSE] - df_ml['Year_Start_Price']) / df_ml['Year_Start_Price']
            df_ml['Institutional_Transactions'] = df_ml.groupby(TICKER)['Shares (Diluted)'].pct_change()
            df_ml['Inst_Inside_Buy'] = df_ml['Institutional_Transactions'].fillna(0)
            
            main_perf = df_ml['Perf_Year'].replace(0, np.nan).combine_first(df_ml['Perf_Half'].replace(0, np.nan))
            df_ml['Momentum_12M_1M'] = df_ml['Perf_Year'] - df_ml['Perf_Month']
            df_ml['Momentum_6M_1M'] = df_ml['Perf_Half'] - df_ml['Perf_Month']

            def cal_rsi(s, n=14):
                diff = s.diff()
                up = diff.clip(lower=0).rolling(n).mean()
                down = -diff.clip(upper=0).rolling(n).mean()
                return 100 - (100 / (1 + (up / down.replace(0, np.nan))))
            df_ml['RSI_14'] = df_ml.groupby(TICKER)[CLOSE].transform(cal_rsi)
            df_ml['RSI_Volatility_Adj'] = df_ml['RSI_14'] / (df_ml['Vol_Month'] + 0.1)

            # Sales_Growth_Proxy ê³„ì‚° ì‹œ EPS_Growth_Next_Year ì°¸ì¡° ì—ëŸ¬ ë°©ì§€
            next_growth = df_ml['EPS_Growth_Next_Year'] if 'EPS_Growth_Next_Year' in df_ml.columns else df_ml['EPS_Growth']
            df_ml['Sales_Growth_Proxy'] = (df_ml['Sales_Growth'].fillna(0) * 0.3) + \
                                         (next_growth.fillna(0) * 0.4) + \
                                         ((df_ml['Perf_Month']+df_ml['Perf_Week'])/2).fillna(0) * 0.3
            
            fwd_earnings_proxy = df_ml['Net Income'] * (1 + df_ml['EPS_Growth'].fillna(0))
            df_ml['Forward_PE'] = m_safe / fwd_earnings_proxy.replace(0, np.nan)
            df_ml['Forward_PE'] = df_ml['Forward_PE'].fillna(df_ml['P/E'])

            target_col = 'Next_Return'
            existing_features = [col for col in selected_features if col in df_ml.columns]

            for col in existing_features:
                df_ml[col] = df_ml.groupby('Industry')[col].transform(lambda x: x.fillna(x.median()))
                df_ml[col] = df_ml[col].fillna(df_ml[col].median())
            
            for col in existing_features:
                df_ml[col] = df_ml.groupby('Industry')[col].rank(pct=True)
            
            df_ml[existing_features] = df_ml[existing_features].fillna(0.5)
            df_ml = df_ml.dropna(subset=[target_col])
            df_ml[existing_features] = df_ml[existing_features].replace([np.inf, -np.inf], 0.5)

            # [ìˆ˜ì • í¬ì¸íŠ¸ 4] ìµœì¢… ë¦¬ë°¸ëŸ°ì‹± ë‚ ì§œ ì •ê·œí™” ë° ì¤‘ë³µ ì œê±° (íšŸìˆ˜ ì •ìƒí™”ì˜ í•µì‹¬)
            # ë‚ ì§œë¥¼ í•´ë‹¹ ë¶„ê¸°ì˜ ë§ˆì§€ë§‰ ë‚ ë¡œ í†µì¼í•˜ì—¬ 'íŒŒíŽ¸í™”ëœ ë‚ ì§œ'ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
            df_ml[REPORT_DATE] = pd.to_datetime(df_ml[REPORT_DATE]).dt.to_period(reb_freq[0]).dt.to_timestamp()
            df_ml = df_ml.sort_values([TICKER, REPORT_DATE]).drop_duplicates(subset=[TICKER, REPORT_DATE], keep='last')

            st.subheader("ðŸ” ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬")
            st.write(f"âœ… ìµœì¢… ìœ íš¨ ë¦¬ë°¸ëŸ°ì‹± ì‹œì  ìˆ˜: {len(df_ml[REPORT_DATE].unique())}íšŒ")
            data_counts = df_ml[existing_features + [target_col]].count().reset_index()
            st.dataframe(data_counts)

            # --- 5ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ---
            if len(df_ml) > 10:
                X = df_ml[existing_features]
                y = df_ml[target_col]
                if isinstance(y, pd.DataFrame): y = y.iloc[:, 0]

                model = RandomForestRegressor(n_estimators=100, random_state=42)
                model.fit(X, y)
                preds = model.predict(X)
                df_ml['Pred'] = preds[:, 0] if preds.ndim > 1 else preds
            else:
                st.error("ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
                st.stop()

            # --- 6ë‹¨ê³„: ë°±í…ŒìŠ¤íŠ¸ ì‹œë®¬ë ˆì´ì…˜ ---
            daily_returns = []
            dates = sorted(df_ml[REPORT_DATE].unique())
            
            for i in range(len(dates)-1):
                cur_date, nxt_date = dates[i], dates[i+1]
                current_pool = df_ml[df_ml[REPORT_DATE] == cur_date]
                if current_pool.empty: continue
                
                top_tickers = current_pool.nlargest(top_n, 'Pred')[TICKER].tolist()
                # ê³µì‹œ ì‹œì°¨ ê³ ë ¤: ë¦¬ë°¸ëŸ°ì‹± ê¸°ì¤€ì¼ + 2ì¼ ë’¤ë¶€í„° ë§¤ìˆ˜ ì‹œìž‘
                trade_start = cur_date + pd.Timedelta(days=2)
                
                period_p = df_prices[(df_prices[TICKER].isin(top_tickers)) & (df_prices[DATE] >= trade_start) & (df_prices[DATE] <= nxt_date)]
                
                if not period_p.empty:
                    daily_pct = period_p.pivot(index=DATE, columns=TICKER, values=CLOSE).pct_change().mean(axis=1)
                    daily_returns.append(daily_pct.dropna())

            if daily_returns:
                df_daily_res = pd.concat(daily_returns)
                df_daily_res = df_daily_res[~df_daily_res.index.duplicated(keep='first')]
                df_cumulative = (1 + df_daily_res).cumprod()

                st.success("ë¶„ì„ ì™„ë£Œ!")
                col1, col2, col3 = st.columns(3)
                col1.metric("ëˆ„ì  ìˆ˜ìµë¥ ", f"{(df_cumulative.iloc[-1]-1)*100:.2f}%")
                col2.metric("MDD", f"{((df_cumulative - df_cumulative.cummax())/df_cumulative.cummax()).min()*100:.2f}%")
                col3.metric("ë¦¬ë°¸ëŸ°ì‹± íšŸìˆ˜", f"{len(dates)}íšŒ")

                tab1, tab2 = st.tabs(["ðŸ“ˆ ìˆ˜ìµë¥  ì¶”ì´", "ðŸ“Š ì¤‘ìš” ì§€í‘œ"])
                with tab1:
                    fig = go.Figure()
                    fig.add_trace(go.Scatter(x=df_cumulative.index, y=df_cumulative.values, name="ì „ëžµ"))
                    st.plotly_chart(fig, use_container_width=True)
                with tab2:
                    # 1. ë°ì´í„° ì¤€ë¹„: ì¤‘ìš”ë„ ê³„ì‚° ë° ëˆ„ì  í•©ê³„ ì¶”ê°€
                    feat_imp = pd.DataFrame({'ì§€í‘œëª…': existing_features, 'ì¤‘ìš”ë„': model.feature_importances_}).sort_values('ì¤‘ìš”ë„', ascending=False)
                    feat_imp['ëˆ„ì  ì¤‘ìš”ë„'] = feat_imp['ì¤‘ìš”ë„'].cumsum()
                    
                    # 2. íŒŒë ˆí†  ì°¨íŠ¸ ê·¸ë¦¬ê¸° (ë§‰ëŒ€ + ë¼ì¸)
                    fig_pareto = go.Figure()

                    # ë§‰ëŒ€ ê·¸ëž˜í”„ (ê°œë³„ ì¤‘ìš”ë„)
                    fig_pareto.add_trace(go.Bar(x=feat_imp['ì§€í‘œëª…'], y=feat_imp['ì¤‘ìš”ë„'], name='ê°œë³„ ì¤‘ìš”ë„', marker_color='rgb(55, 83, 109)'))

                    # ì„  ê·¸ëž˜í”„ (ëˆ„ì  ì¤‘ìš”ë„)
                    fig_pareto.add_trace(go.Scatter(x=feat_imp['ì§€í‘œëª…'], y=feat_imp['ëˆ„ì  ì¤‘ìš”ë„'], name='ëˆ„ì  ì¤‘ìš”ë„', yaxis='y2', line=dict(color='rgb(219, 64, 82)', width=3)))

                    # ë ˆì´ì•„ì›ƒ ì„¤ì • (ì´ì¤‘ ì¶• ì ìš©)
                    fig_pareto.update_layout(
                        title='ì§€í‘œ ì¤‘ìš”ë„ íŒŒë ˆí†  ë¶„ì„ (Feature Importance Pareto)',
                        xaxis=dict(title='Financial Metrics'),
                        yaxis=dict(title='ê°œë³„ ì¤‘ìš”ë„', showgrid=True),
                        yaxis2=dict(
                            title='ëˆ„ì  ì¤‘ìš”ë„', overlaying='y', side='right', range=[0, 1.05], tickformat='.0%', showgrid=False),
                        legend=dict(x=0.8, y=1.2, orientation='h'),
                        margin=dict(l=50, r=50, t=80, b=50),
                        hovermode='x unified'
                    )
                    
                    st.plotly_chart(fig_pareto, use_container_width=True)

                    # 3. ìƒì„¸ ì¤‘ìš”ë„ í…Œì´ë¸” ì¶”ê°€
                    st.markdown("---")
                    st.subheader("ðŸ“Š ì§€í‘œë³„ ìƒì„¸ ì¤‘ìš”ë„ í…Œì´ë¸”")
                    
                    # í‘œì‹œìš© ë°ì´í„°í”„ë ˆìž„ í¬ë§·íŒ…
                    display_df = feat_imp.copy()
                    display_df['ì¤‘ìš”ë„ ë¹„ì¤‘'] = display_df['ì¤‘ìš”ë„'].apply(lambda x: f"{x:.2%}")
                    display_df['ëˆ„ì  ë¹„ì¤‘'] = display_df['ëˆ„ì  ì¤‘ìš”ë„'].apply(lambda x: f"{x:.2%}")
                    
                    # í…Œì´ë¸” ì¶œë ¥
                    st.table(display_df[['ì§€í‘œëª…', 'ì¤‘ìš”ë„ ë¹„ì¤‘', 'ëˆ„ì  ë¹„ì¤‘']])
            else:
                st.warning("ì¡°ê±´ì— ë§žëŠ” ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")